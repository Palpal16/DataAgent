# ================================================================================
# DATA AGENT CONFIGURATION
# ================================================================================
# Configuration for the Sales Data Agent with evaluation and batch processing.
# All options map directly to data_agent.py command-line arguments.
#
# Quick Start:
#   1. Set your prompt and agent_mode
#   2. For evaluation, provide gt_csv and/or gt_text and enable evaluation
#   3. For batch processing, provide test_cases_json
# ================================================================================

# ================================================================================
# CORE SETTINGS
# ================================================================================

# User's natural language prompt/question
prompt: "average yearly sales"

# Path to parquet data file (null = use default)
data_path: null

# Optional visualization goal (if different from prompt)
visualization_goal: null

# ================================================================================
# PYTHON
# ================================================================================

# Python executable used by the C++ config runner
#python_bin: "/mnt/c/Users/Recup/Documents/Polimi/PACS/DataAgent/.venv/Scripts/python.exe"
python_bin: "python"
# ================================================================================
# MODEL CONFIGURATION
# ================================================================================

# Ollama model name (e.g., llama3.2:3b)
model: "llama3.2:3b"

# Ollama server URL
ollama_url: "http://localhost:11434"

temperature: 0.1

# ================================================================================
# AGENT BEHAVIOR
# ================================================================================

# Agent mode controls the pipeline:
#   lookup_only: Only fetch data (fastest)
#   analysis:    Fetch data + analyze (no visualization)
#   full:        Complete pipeline with visualization code
agent_mode: "analysis"

# ================================================================================
# BEST-OF-N SAMPLING
# ================================================================================

# Run agent N times and pick the best result based on evaluation scores
# Requires evaluation to be enabled (gt_csv and/or gt_text)
best_of_n: 1

# Temperature range for best-of-n sampling
# If best_of_n > 1, temperatures are linearly spaced between temp and temp_max
# Set temp_max to null to use the same temperature for all runs
temperature_max: null

# ================================================================================
# OUTPUT SETTINGS
# ================================================================================

# Directory to save results (CSV, analysis, all_results.json)
# If null, creates a temporary directory
save_dir: ./output

# ================================================================================
# GROUND TRUTH (Single Query Evaluation)
# ================================================================================

# Path to ground-truth CSV file (as string or file)
gt_csv: null

# Path to ground-truth text file for analysis comparison
gt_text: null

# ================================================================================
# CSV EVALUATION
# ================================================================================

# Enable CSV evaluation (requires gt_csv)
enable_csv_eval: true

# CSV evaluation method
csv_eval_method: "python"  # Options: python | cpp

# IoU type for CSV comparison
csv_iou_type: "rows"  # Options: rows | columns | table

# C++ evaluator settings (only used if csv_eval_method = cpp)
cpp_evaluator:
  executable: null  # Path to C++ evaluator binary
  keys: null        # Comma-separated key columns (e.g., "store_id,date")

# ================================================================================
# TEXT EVALUATION
# ================================================================================

# Enable text evaluation (requires gt_text)
enable_text_eval: true

# Text evaluation method
# Options: bleu | spice | llm
text_eval_method: "bleu"

# BLEU settings
bleu:
  use_nltk: false  # Use NLTK BLEU instead of simple implementation

# SPICE settings (for text_eval_method = spice)
spice:
  jar_path: null     # Path to SPICE jar file (required for SPICE)
  java_bin: "java"   # Java executable path

# LLM Judge settings (for text_eval_method = llm)
llm_judge:
  model: null  # Model to use as judge (null = use main model)

# ================================================================================
# BATCH PROCESSING (JSON Test Cases)
# ================================================================================

# Path to JSON file containing test cases
# JSON format: [{
#   "prompt": "...",
#   "gt_data": "...csv string...",    # Used as gt_csv
#   "gt_analysis": "...",              # Used as gt_text
#   "gt_sql": "..."                    # Optional, for reference only
# }, ...]
test_cases_json: ./evaluation/our_dataset.json

# Run in batch mode (process all test cases in JSON file)
# When true, ignores single prompt/gt_csv/gt_text settings
run_batch: true

# ================================================================================
# TRACING & MONITORING
# ================================================================================

# Enable Phoenix/OpenInference tracing
enable_tracing: false

# Phoenix configuration
phoenix:
  endpoint: "http://localhost:6006/v1/traces"
  project_name: "evaluating-agent"
  api_key: null  # Optional API key for cloud Phoenix
  auto_start: true

# ================================================================================
# ENERGY TRACKING
# ================================================================================

# Enable CodeCarbon energy/emissions tracking
enable_codecarbon: false